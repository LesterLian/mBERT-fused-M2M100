{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scores.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVbk-IwuorIa"
      },
      "source": [
        "# Load Dataset from Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6MJuTvGo1LI"
      },
      "source": [
        "!pip install bert_score\n",
        "!pip install jiwer\n",
        "!pip install transfomers\n",
        "!pip install datasets\n",
        "#pip install git+https://github.com/google-research/bleurt.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q014ofvKzuN_"
      },
      "source": [
        "!pip3 install sacrebleu"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKGAFkuFpH-z"
      },
      "source": [
        "#data in bytes format\n",
        "orig_en_data_b = open(\"/content/drive/MyDrive/orig_english.en\",\"r\")\n",
        "model_prediction_b = open(\"/content/drive/MyDrive/pred_finetune_M2M_decoder.en\",\"r\")\n",
        "#model_prediction_b = open(\"/content/drive/MyDrive/prediction.en\",\"r\")\n",
        "m2m_prediction_b=open(\"/content/drive/MyDrive/pred_english_fusedmodel.en\",\"r\")\n",
        "\n",
        "#string conversion\n",
        "orig_en_data = [line for line in orig_en_data_b]\n",
        "model_prediction = [line for line in model_prediction_b]\n",
        "m2m_prediction= [line for line in m2m_prediction_b]\n",
        "\n",
        "#list conversion\n",
        "m2m_prediction_bleu =[]\n",
        "for i in m2m_prediction:\n",
        "  m2m_prediction_bleu.append([i])\n",
        "\n",
        "orig_en_data_bleu =[]\n",
        "for i in orig_en_data:\n",
        "  orig_en_data_bleu.append([i])\n",
        "\n",
        "model_prediction_bleu =[]\n",
        "for i in orig_en_data:\n",
        "  model_prediction_bleu.append([i])  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IY8UetlgpfF5"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "#from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"sacrebleu\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "print(\"original file length: \",len(orig_en_data_bleu))\n",
        "print(\"prediction file length: \",len(model_prediction))\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_bleu)\n",
        "print(\"Fused value: \",final_sacrebleu_score)\n",
        "#print(np.average(final_sacrebleu_score['scores']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOu67os-LUUf"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGlBcGrYOXkM"
      },
      "source": [
        "pip install unbabel-comet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dU5nHdAMPQZ3"
      },
      "source": [
        "!pip install torchtext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SO-2PWLuR2di"
      },
      "source": [
        "!git clone https://github.com/Unbabel/COMET"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1OmSNt_SG0O"
      },
      "source": [
        "!pip install --user poetry"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whcIf5nNSOgU"
      },
      "source": [
        "comet score -h hyp.en -r ref.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzZTlPpbORFQ"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "#from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"gleu\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "print(\"original file length: \",len(orig_en_data_bleu))\n",
        "print(\"prediction file length: \",len(model_prediction))\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_bleu)\n",
        "print(\"Fused value: \",final_sacrebleu_score)\n",
        "#print(np.average(final_sacrebleu_score['scores']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVqGwmjxujep"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "#from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"wer\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data)\n",
        "print(\"Fused value: \",final_sacrebleu_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFSzIuJW6_ZY"
      },
      "source": [
        "!pip install git+https://github.com/google-research/bleurt.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4-NFwiiuloh"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "#from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"bleurt\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data)\n",
        "score=np.average(final_sacrebleu_score['scores'])\n",
        "print(\"Fused value: \",score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcVBLfGJ8uLX"
      },
      "source": [
        "!pip install -U nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4nrN2uKur_H"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"meteor\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data)\n",
        "print(\"Fused value: \",final_sacrebleu_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47-gbVHuxRZ"
      },
      "source": [
        "#BLEU calculation for M2M-100 = 14.328622681457187\n",
        "#original english test data\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from nltk.translate import meteor_score \n",
        "\n",
        "sacrebleu_metric = load_metric(\"cer\")\n",
        "\n",
        "print(model_prediction)\n",
        "print(orig_en_data)\n",
        "'''\n",
        "orig_en_data_tmp = []\n",
        "\n",
        "for i in orig_en_data:\n",
        "     orig_en_data_tmp.append([i])\n",
        "\n",
        "print(orig_en_data_tmp)\n",
        "\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data_tmp)\n",
        "'''\n",
        "final_sacrebleu_score = sacrebleu_metric.compute(predictions=model_prediction, references=orig_en_data)\n",
        "print(\"Fused value: \",final_sacrebleu_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQfff1hbtOur"
      },
      "source": [
        "!python3 chrF++.py -R orig_english.en -H pred_finetune_M2M_decoder.en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_zwHuxjtWyr"
      },
      "source": [
        "% cd CharacTER/\n",
        "!python3 CharacTER.py -r \"/content/drive/My Drive/orig_english.en\" -o \"/content/drive/My Drive/pred_finetune_M2M_decoder.en\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}